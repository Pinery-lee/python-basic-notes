# Self-attention å’Œ Multi-head attention

## 0. é—®é¢˜èƒŒæ™¯

éšç€è®¡ç®—æœºè§†è§‰ä¸­transformeræ¨¡å‹æ‰€å çš„æ¯”ä¾‹è¶Šæ¥è¶Šé«˜ï¼Œä»¥åŠç«çƒ­çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ŒåŸºäºtransformerçš„è§†è§‰æ¨¡å‹ä¸åŸºäºcnnçš„æ¨¡å‹å‘ˆç°åˆ†åº­æŠ—ç¤¼çš„æ€åŠ¿ã€‚æ‰€ä»¥ä»[Attention Is All You Need](https://yiyibooks.cn/yiyibooks/Attention_Is_All_You_Need/index.html)è¯¥æ–‡ä¸­å­¦ä¹ è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æˆäº†cvä»ä¸šè€…çš„åŸºæœ¬åŠŸã€‚è¯¥æ–‡ä½¿ç”¨è‡ªæ³¨æ„åŠ›çš„åŠ¨æœºåœ¨äºï¼šä¼ ç»ŸåŸºäºRNNæˆ–è€…CNNçš„æ¨¡å‹åœ¨å­¦ä¹ è¿œè·ç¦»ä¾èµ–æ—¶(å¤§çš„å·ç§¯æ ¸æˆ–è€…æ›´å¤šå±‚)è®¡ç®—ä»£ä»·è¾ƒé«˜ï¼ŒTransformer é€šè¿‡ self-attention å°†å…¶é™ä½ä¸ºå¸¸æ•°å¤æ‚åº¦ï¼Œä½†å¼•å…¥äº†å¹³å‡æ¨¡ç³Šï¼ˆå› ä¸º attention å®è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒå¹³å‡ï¼ˆweighted sumï¼‰æ“ä½œï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¿¡æ¯â€œæ¨¡ç³ŠåŒ–â€ã€‚å¦‚æœ attention æŠŠæ³¨æ„åŠ›åˆ†é…å¾—è¿‡äºå¹³å‡ï¼Œé‡è¦çš„ä¿¡æ¯å¯èƒ½ä¼šè¢«å…¶ä»–ä¸ç›¸å…³ä½ç½®â€œç¨€é‡Šâ€ã€‚ï¼‰çš„é—®é¢˜ï¼Œè¿™ä¸€é—®é¢˜å¯ä»¥ç”¨ Multi-Head Attention æ¥ç¼“è§£ã€‚

## 1. è‡ªæ³¨æ„åŠ›

è‡ªæ³¨æ„åŠ›ï¼Œåˆç§°ä¸ºç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¯åº”ç”¨æœ€å¹¿æ³›çš„çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚

> åŠ¨æœºï¼šè‡ªæ³¨æ„åŠ›æå‡ºçš„åŠ¨æœºæ˜¯ä¸ºäº†å‡è½»æˆ–è€…æ¶ˆé™¤è‡ªç„¶è¯­è¨€å¤„ç†ä¸­RNNæ¨¡å‹çš„å¹¶è¡ŒåŒ–å’Œé•¿ç¨‹ä¾èµ–é—®é¢˜ã€‚

å¯¹æ¯”ï¼š

| **ç‰¹æ€§**         | **è‡ªæ³¨æ„åŠ› (Self-Attention)**    | **RNN**                        | **CNN**                      |
| ---------------- | -------------------------------- | ------------------------------ | ---------------------------- |
| **ä¾èµ–å…³ç³»å»ºæ¨¡** | å…¨å±€ä¾èµ–ï¼ˆä»»æ„ä½ç½®é—´ç›´æ¥äº¤äº’ï¼‰   | å±€éƒ¨ä¾èµ–ï¼ˆä»…å†å²ä¿¡æ¯é€æ­¥ä¼ é€’ï¼‰ | å±€éƒ¨ä¾èµ–ï¼ˆå›ºå®šçª—å£å†…äº¤äº’ï¼‰   |
| **å¹¶è¡ŒåŒ–èƒ½åŠ›**   | å®Œå…¨å¹¶è¡Œï¼ˆçŸ©é˜µè¿ç®—ï¼‰             | åºåˆ—ä¾èµ–ï¼Œéš¾ä»¥å¹¶è¡Œ             | éƒ¨åˆ†å¹¶è¡Œï¼ˆçª—å£å†…å¯å¹¶è¡Œï¼‰     |
| **é•¿ç¨‹ä¾èµ–å¤„ç†** | å¤©ç„¶æ”¯æŒé•¿ç¨‹ä¾èµ–ï¼ˆæ— è¡°å‡ï¼‰       | æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜ä¸¥é‡          | éœ€å †å å¤šå±‚æ‰©å¤§æ„Ÿå—é‡         |
| **è®¡ç®—å¤æ‚åº¦**   | $O(n^2)$ ï¼ˆåºåˆ—é•¿åº¦ \(n\)ï¼‰      | $O(n)$ ï¼ˆæ—¶é—´æ­¥è®¡ç®—ï¼‰          | $O(k \cdot n)$ ï¼ˆæ ¸å¤§å°kï¼‰   |
| **å‚æ•°å…±äº«**     | æ— ä½ç½®å‚æ•°å…±äº«ï¼ˆä½†å¤šå¤´å…±äº«æƒé‡ï¼‰ | æ—¶é—´æ­¥å…±äº«æƒé‡                 | ç©ºé—´ä½ç½®å…±äº«å·ç§¯æ ¸           |
| **å…¸å‹åº”ç”¨åœºæ™¯** | Transformerã€BERTã€GPT           | è¯­è¨€æ¨¡å‹ã€æ—¶é—´åºåˆ—é¢„æµ‹         | å›¾åƒåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»ï¼ˆ1Då·ç§¯ï¼‰ |

### 1.1 ç†è®ºå…¬å¼


$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$


-   $(Q, K, V)$ ä¸ºæŸ¥è¯¢Queryã€é”®Keyã€å€¼ValueçŸ©é˜µï¼Œ $ \sqrt{d_k} $ ä¸ºç¼©æ”¾å› å­ï¼Œ $d_k$ æ˜¯Keyçš„ç»´åº¦ã€‚
-   $(Q, K, V)$ è¡¨ç¤ºæ¯ä¸ª token çš„ Queryâ€œè¦å…³æ³¨ä»€ä¹ˆâ€ã€Keyâ€œåŒ¹é…ä»€ä¹ˆâ€ï¼Œå’Œ Valueâ€œè¾“å‡ºä»€ä¹ˆâ€ï¼Œç”±äº $(Q, K, V)$ **éƒ½æ˜¯æ¥è‡ªäºè¾“å…¥**ï¼Œæ‰€ä»¥å«**è‡ªæ³¨æ„åŠ›**æœºåˆ¶
-  ç¬¬ä¸€æ­¥é€šè¿‡**ç‚¹ç§¯**è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼ˆattention scoreï¼‰ï¼šç‚¹ç§¯ $QK^T$ å°±æ˜¯è®¡ç®— $Q$ å’Œ $K$ çš„ç›¸ä¼¼åº¦ã€‚
-  ç¬¬äºŒæ­¥å°†æ³¨æ„åŠ›å¾—åˆ†è¿›è¡Œ**ç¼©æ”¾**å’Œ**å½’ä¸€åŒ–**ï¼šç¼©æ”¾å°±æ˜¯é™¤ä»¥ $\sqrt{d_k}$ , å½’ä¸€åŒ–å°±æ˜¯ä½¿ç”¨ $softmax$ ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼ˆattention weight)ã€‚è¿™ä¹Ÿæ˜¯ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›åç§°çš„æ¥æºã€‚
-  ç¬¬ä¸‰æ­¥ä½¿ç”¨æ³¨æ„åŠ›æƒé‡å¯¹ $V$ è¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°è¾“å‡ºã€‚

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦QKV?

**æˆ‘ä»¬éœ€è¦ QKVï¼ˆQueryã€Keyã€Valueï¼‰æœºåˆ¶ï¼Œè€Œä¸æ˜¯ç›´æ¥å­¦ä¹ æ³¨æ„åŠ›æƒé‡ï¼Œæ˜¯ä¸ºäº†è®©æ³¨æ„åŠ›å…·æœ‰**ï¼š
 âœ… å¯æ³›åŒ–æ€§ã€âœ… åŠ¨æ€æ€§ã€âœ… è®¡ç®—æ•ˆç‡é«˜ã€âœ… æ”¯æŒé•¿åºåˆ—ã€âœ… å‚æ•°é‡å°ã€‚

####  1.2.1 å¦‚æœ**ç›´æ¥å­¦ä¹ æ³¨æ„åŠ›æƒé‡**ä¼šæ€ä¹ˆæ ·ï¼Ÿ

- è¿™æ„å‘³ç€ä½ è¦**ä¸ºæ¯ä¸ª token å­¦ä¸€ä¸ªæƒé‡å€¼**
- æƒé‡æ˜¯å›ºå®šçš„ï¼Œè·Ÿè¾“å…¥å†…å®¹æ— å…³
- å¯¹ä¸åŒè¾“å…¥æ ·æœ¬åªèƒ½ä½¿ç”¨**åŒä¸€å¥—æ³¨æ„åŠ›è¿æ¥** â†’ ä¸èƒ½æ³›åŒ–
- å‚æ•°é‡å·¨å¤§ï¼Œä¸æ”¯æŒå˜åŒ–é•¿åº¦çš„åºåˆ—
- è¿™æœ¬è´¨ä¸Šæ˜¯ï¼šä¸€ä¸ª **è¶…å¤§çš„å…¨è¿æ¥å±‚æˆ–å·ç§¯æ ¸**

####  1.2.2 ä½¿ç”¨ QKV çš„è‡ªæ³¨æ„åŠ›æ˜¯æ€ä¹ˆåšçš„ï¼Ÿ

æ ¸å¿ƒæ€è·¯ï¼š $Q = XW_q^T+B, K = XW_k^T+B, V = XW_v^T+B$  â†’ QKV æ˜¯ä»è¾“å…¥é€šè¿‡çº¿æ€§å˜æ¢åŠ¨æ€ç”Ÿæˆçš„ï¼

```python
self.query = nn.Linear(self.hidden_size, self.all_head_size, bias=config.qkv_bias)
self.key = nn.Linear(self.hidden_size, self.all_head_size, bias=config.qkv_bias)
self.value = nn.Linear(self.hidden_size, self.all_head_size, bias=config.qkv_bias)
```

> âœ… **åœ¨è®­ç»ƒä¸­ï¼Œæ¨¡å‹çœŸæ­£å­¦ä¹ çš„å‚æ•°æ˜¯ç”Ÿæˆ Qã€Kã€V çš„æƒé‡çŸ©é˜µ** â€”â€” å³  $ W_q, W_k, W_v$ ï¼Œè€Œä¸æ˜¯æ³¨æ„åŠ›æƒé‡æœ¬èº«ã€‚æ³¨æ„åŠ›æƒé‡å¹¶ä¸æ˜¯ç›´æ¥å­¦å¾—çš„å‚æ•°ï¼Œè€Œæ˜¯é€šè¿‡è¾“å…¥è®¡ç®—å‡ºæ¥çš„ä¸­é—´ç»“æœã€‚

| åç§°                | å‚æ•°æ€§è´¨   | è¯´æ˜                                 |
| ------------------- | ---------- | ------------------------------------ |
| `W_Q`               | âœ… å¯å­¦ä¹    | æŠŠè¾“å…¥æŠ•å½±æˆ Query ç©ºé—´              |
| `W_K`               | âœ… å¯å­¦ä¹    | æŠŠè¾“å…¥æŠ•å½±æˆ Key ç©ºé—´                |
| `W_V`               | âœ… å¯å­¦ä¹    | æŠŠè¾“å…¥æŠ•å½±æˆ Value ç©ºé—´              |
| `attention_weights` | âŒ ä¸æ˜¯å‚æ•° | æ˜¯ç”± Q å’Œ K è®¡ç®—å‡ºæ¥çš„å†…å®¹ç›¸å…³æ€§ç»“æœ |

ä½¿ç”¨QKVè®¡ç®—æ³¨æ„åŠ›æƒé‡çš„å¥½å¤„ï¼š

| ç‰¹æ€§             | QKVè‡ªæ³¨æ„åŠ›                       |
| ---------------- | --------------------------------- |
| **åŠ¨æ€æ€§**       | Qã€Kã€V æ˜¯æ¯ä¸ªè¾“å…¥åŠ¨æ€ç”Ÿæˆçš„      |
| **å¯æ³›åŒ–æ€§**     | åŒä¸€å¥—å‚æ•° W_q/k/v å¯ç”¨äºä¸åŒè¾“å…¥ |
| **å‚æ•°é‡å°**     | åªéœ€å­¦ä¹  W_q/k/vï¼ˆ3 Ã— dÃ—dï¼‰       |
| **æ”¯æŒå˜é•¿è¾“å…¥** | $QK^T$  è‡ªåŠ¨é€‚é…ä¸åŒåºåˆ—é•¿åº¦      |
| **å†…å®¹æ„ŸçŸ¥æ€§**   | ä¸åŒè¾“å…¥äº§ç”Ÿä¸åŒæ³¨æ„åŠ›çŸ©é˜µ        |

#### 1.2.3 ä¸¾ä¾‹è¯´æ˜

å‡è®¾ä½ æ­£åœ¨çœ‹ä¸€å¥è¯ï¼šâ€œ**The cat sat on the mat.**â€

ä½¿ç”¨ QKVï¼š

- æ¯ä¸ªè¯ï¼ˆtokenï¼‰å…ˆç”Ÿæˆä¸€ä¸ª Queryã€Keyã€Value å‘é‡
- Query å’Œæ‰€æœ‰ Key ç‚¹ç§¯ï¼Œå¾—åˆ°æ¯ä¸ªè¯å¯¹å½“å‰è¯çš„é‡è¦ç¨‹åº¦
- å†ç”¨è¿™äº›æƒé‡åŠ æƒ Valueï¼Œå¾—åˆ°å½“å‰è¯çš„è¯­ä¹‰è¡¨ç¤º

æ¯æ¬¡è¾“å…¥å†…å®¹å˜äº†ï¼ŒQã€Kã€V éƒ½å˜äº†ï¼Œæ³¨æ„åŠ›æƒé‡ä¹Ÿä¼šè·Ÿç€å˜ã€‚è¿™æ˜¯â€œ**å†…å®¹æ„ŸçŸ¥çš„è¿æ¥æƒé‡**â€ã€‚

#### 1.2.4 ç®€è¦æ€»ç»“

| é—®é¢˜                          | å›ç­”ç®€è¦è¯´æ˜                               |
| ----------------------------- | ------------------------------------------ |
| ä¸ºä»€ä¹ˆä¸ç”¨ç›´æ¥å­¦ä¹ æƒé‡ï¼Ÿ      | ä¼šå›ºå®šã€æ— æ³•æ³›åŒ–ã€å‚æ•°å¤ªå¤šã€è¾“å…¥ä¸èƒ½å˜     |
| QKV æœ‰ä»€ä¹ˆç”¨ï¼Ÿ                | æä¾›åŠ¨æ€æƒé‡ã€æ”¯æŒå˜é•¿è¾“å…¥ã€å‚æ•°å°‘ã€å¯æ³›åŒ– |
| æ˜¯ä¸æ˜¯ QKV æœ€åä¹Ÿä¼šå˜æˆæƒé‡ï¼Ÿ | æ˜¯ï¼Œä½†è¿™äº›æƒé‡æ˜¯**è¾“å…¥é©±åŠ¨ã€å†…å®¹æ„ŸçŸ¥çš„**   |

### 1.3 ä¸ºä»€ä¹ˆè¦é™¤ä»¥ $\sqrt{d_k}$ ï¼Ÿ

æ€»ç»“ä¸€å¥è¯ï¼šç¼©æ”¾æ˜¯å¯ä»¥ä½¿å¾—æ³¨æ„åŠ›åˆ†æ•°å³ç‚¹ç§¯ç»“æœä¸è‡³äºå¾ˆå¤§ï¼Œè¿™æ ·ç»è¿‡softmaxå½’ä¸€åŒ–åä¸ä¼šå€¾å‘äºå…¶ä¸­æœ€å¤§çš„å…ƒç´ æ¥è¿‘1ï¼Œå…¶ä½™å…¨æ˜¯0çš„åˆ†å¸ƒï¼Œè¿™ç§åˆ†å¸ƒä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚è¯¦ç»†è®²è§£å¯ä»¥çœ‹[è¿™é‡Œ](https://zhuanlan.zhihu.com/p/695762892?share_code=w1h4U3lhaCOa&utm_psn=1903852051466785745)

é™¤ä»¥æ ‡å‡†å·® $\sqrt{d_k}$ ï¼Œå°±æ˜¯ä¸ºäº†æŠŠç‚¹ç§¯ç»“æœçš„æ–¹å·®å˜ä¸º1ï¼ˆç±»ä¼¼äºæ™®é€šæ­£æ€åˆ†å¸ƒå˜ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œä¸è¿‡æœŸæœ›ä¸º0ï¼‰,å®éªŒä¸€ä¸‹ï¼š

```python
import torch
import numpy as np

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
torch.manual_seed(0)

# è®¾ç½®å‘é‡ç»´åº¦å’Œæ ·æœ¬æ•°é‡
d_k = 64
num_samples = 100000

# ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å‘é‡å’Œé”®å‘é‡
queries = torch.randn(num_samples, d_k)  # å½¢çŠ¶: (num_samples, d_k)
keys = torch.randn(num_samples, d_k)     # å½¢çŠ¶: (num_samples, d_k)

# è®¡ç®—æ¯ä¸€å¯¹æŸ¥è¯¢å‘é‡å’Œé”®å‘é‡çš„ç‚¹ç§¯
dot_products = torch.sum(queries * keys, dim=1)  # å½¢çŠ¶: (num_samples,)

# è®¡ç®—ç‚¹ç§¯çš„æ–¹å·®ï¼ˆæ€»ä½“æ–¹å·®ï¼‰
variance1 = torch.var(dot_products / np.sqrt(d_k), unbiased=False)
variance2 = torch.var(dot_products , unbiased=False)

print(f"ç¼©æ”¾åç‚¹ç§¯çš„æ–¹å·®ï¼ˆå¤§æ ·æœ¬é‡ï¼‰: {variance1}")
print(f"ç¼©æ”¾å‰ç‚¹ç§¯çš„æ–¹å·®ï¼ˆå¤§æ ·æœ¬é‡ï¼‰: {variance2}")
```

è¾“å‡ºï¼š

```python
ç¼©æ”¾åç‚¹ç§¯çš„æ–¹å·®ï¼ˆå¤§æ ·æœ¬é‡ï¼‰: 0.997921884059906
ç¼©æ”¾å‰ç‚¹ç§¯çš„æ–¹å·®ï¼ˆå¤§æ ·æœ¬é‡ï¼‰: 63.867000579833984
```

### 1.4 ä¸ºä»€ä¹ˆè¦ç”¨softmaxå½’ä¸€åŒ–ï¼Ÿ

> å› ä¸ºæˆ‘ä»¬å¸Œæœ›æ³¨æ„åŠ›æƒé‡æ˜¯**æ¦‚ç‡åˆ†å¸ƒ**ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼šæ¯ä¸ª token å¯¹å…¶ä»– token çš„å…³æ³¨åº¦æ˜¯éè´Ÿçš„ï¼Œæ€»å’Œä¸º 1ï¼Œè¿™æ ·å°±èƒ½è¿›è¡Œ**åŠ æƒå¹³å‡**ï¼Œè€Œä¸æ˜¯ä»»æ„æ”¾å¤§æˆ–ç¼©å°ã€‚

#### 1.4.1  **å°†ä»»æ„çš„æ‰“åˆ†å˜æˆæ¦‚ç‡**

ç‚¹ç§¯åªæ˜¯ä¸€ä¸ªåˆ†æ•°ï¼Œæ²¡æœ‰ç•Œé™ï¼Œå¯ä»¥æ­£ä¹Ÿå¯ä»¥è´Ÿã€‚
 å¦‚æœä¸å½’ä¸€åŒ–ï¼Œå€¼å¯ä»¥å¾ˆå¤§ã€å¾ˆå°ç”šè‡³è´Ÿæ•°ï¼Œä¸é€‚åˆå½“ä½œâ€œæƒé‡â€ã€‚

> softmax æŠŠè¿™äº› raw scores è½¬æ¢ä¸º [0,1][0, 1] çš„æ¦‚ç‡ï¼Œæ€»å’Œä¸º 1
>  â†’ ä¾¿äºæˆ‘ä»¬å¯¹ value å‘é‡åšåŠ æƒå¹³å‡ã€‚

------

#### 1.4.2 å¯è§£é‡Šæ€§ï¼šå½’ä¸€åŒ–ä¹‹åå°±èƒ½è§£é‡Šâ€œå…³æ³¨å¤šå°‘â€

softmax åçš„æ³¨æ„åŠ›æƒé‡ $\alpha_{ij}$ å¯ä»¥ç›´æ¥è§£é‡Šä¸ºï¼š

- â€œç¬¬ i ä¸ª token å¯¹ç¬¬ j ä¸ª token çš„æ³¨æ„åŠ›æœ‰å¤šå°‘â€
- æ‰€ä»¥å¯è§†åŒ–æ—¶ä½ ä¼šçœ‹åˆ°æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼ˆAttention Heatmapï¼‰ï¼Œä¹Ÿéƒ½æ˜¯åŸºäºè¿™ä¸ª softmax æƒé‡

------

#### 1.4.3  **ä¾¿äºæ¢¯åº¦ä¼ æ’­**

softmax æœ‰å¹³æ»‘ç‰¹æ€§ï¼Œé¿å…äº† hard selectionï¼ˆæ¯”å¦‚ argmax é‚£ç§ä¸å¯å¯¼æ“ä½œï¼‰
 å¯ä»¥è®©æ¨¡å‹å­¦ä¼šâ€œæ›´å…³æ³¨è°â€è€Œä¸æ˜¯â€œåªå…³æ³¨è°â€

------

#### 1.4.4 **å½’ä¸€åŒ–åï¼Œå€¼ä¸ä¼šçˆ†ç‚¸ï¼Œæœ‰åˆ©äºè®­ç»ƒç¨³å®š**

å¦‚æœç›´æ¥ç”¨æœªå½’ä¸€åŒ–çš„æ‰“åˆ†æ¥åŠ æƒ valueï¼Œå®¹æ˜“å‡ºç°æ•°å€¼ä¸ç¨³å®šã€æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±

------

ğŸ§  ä¸¾ä¸ªä¾‹å­ï¼š

å‡è®¾ä¸€ä¸ª token å¯¹å…¶ä»– 3 ä¸ª token çš„æ‰“åˆ†æ˜¯ï¼š

[2.3, 9.1, 0.5]

å¦‚æœç›´æ¥æ‹¿è¿™ä¸ªå‘é‡åšæƒé‡æ¥åŠ æƒ valueï¼Œç»“æœéš¾ä»¥è§£é‡Šï¼Œä¸”ä¸ä¸€å®šç¨³å®šã€‚
 ä½† softmax åï¼š

$softmax([2.3,9.1,0.5])â‰ˆ[0.002, 0.995, 0.003]\text{softmax}([2.3, 9.1, 0.5]) \approx [0.002,\ 0.995,\ 0.003]$

è¿™ä¸ªå°±å¯ä»¥è¡¨ç¤ºä¸ºï¼šâ€œæˆ‘å‡ ä¹å…¨éƒ¨æ³¨æ„åŠ›éƒ½ç»™äº†ç¬¬ 2 ä¸ª tokenâ€ï¼Œå¯è§£é‡Šã€ç¨³å®šã€å¯è®­ç»ƒã€‚

------

âœ… æ€»ç»“ä¸€å¥è¯ï¼š

> **å½’ä¸€åŒ–æ˜¯ä¸ºäº†è®©æ³¨æ„åŠ›æƒé‡å˜æˆæ¦‚ç‡åˆ†å¸ƒ**ï¼Œä»è€Œä¾¿äºæ¨¡å‹å­¦ä¹ ã€è§£é‡Šå’Œç¨³å®šåœ°åŠ æƒ value å€¼ã€‚

---

### 1.5 å®ç°

#### 1.5.1 ä¼ªä»£ç 

- ä¼ªä»£ç ç”¨äºè¡¨ç¤ºæ ¸å¿ƒçš„ç²¾ç®€æµç¨‹

- `@`è¡¨ç¤ºçŸ©é˜µä¹˜æ³•ï¼Œé€‚ç”¨äº`torch.tensor`, ç­‰ä»·äº`torch.matmul(a, b)`

```python
# è¾“å…¥ï¼šX âˆˆ â„^(LÃ—d)
Q = X @ W_Q    # W_Q âˆˆ â„^(dÃ—d)
K = X @ W_K
V = X @ W_V
# æ³¨æ„åŠ›å¾—åˆ†ï¼š
attention_scores = Q @ K.T / sqrt(d)
# æ³¨æ„åŠ›æƒé‡ï¼š
attention_weights = softmax(attention_scores, dim=-1)
# æœ€ç»ˆè¾“å‡ºï¼š
output = attention_weights @ V
```

#### 1.5.2 åŸç”Ÿå®ç°

- åŸç”Ÿå®ç°å¯ä»¥è·‘é€šï¼Œä½†æ²¡æœ‰å¤šä½™ä¼˜åŒ–ï¼Œ $QKV$ ä¹Ÿéƒ½æ˜¯éšæœºçš„è€Œä¸æ˜¯ç”±è¾“å…¥çº¿æ€§å˜åŒ–å¾—åˆ°çš„ã€‚
- å·²ç»å°†å…¬å¼å…¨éƒ¨å®ç°

```python
# å¯¼å…¥åº“
import torch
import torch.nn.functional as F

# ç¤ºä¾‹è¾“å…¥åºåˆ—
input_sequence = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9],[0.5, 0.2, 0.4]])

# ç”Ÿæˆ Keyã€Query å’Œ Value çŸ©é˜µçš„éšæœºæƒé‡
random_weights_key = torch.randn(input_sequence.size(-1), input_sequence.size(-1))
random_weights_query = torch.randn(input_sequence.size(-1), input_sequence.size(-1))
random_weights_value = torch.randn(input_sequence.size(-1), input_sequence.size(-1))

# è®¡ç®— Keyã€Query å’Œ Value çŸ©é˜µ
key = torch.matmul(input_sequence, random_weights_key)
query = torch.matmul(input_sequence, random_weights_query)
value = torch.matmul(input_sequence, random_weights_value)

# è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
attention_scores = torch.matmul(query, key.T) / torch.sqrt(torch.tensor(query.size(-1), dtype=torch.float32))

# ä½¿ç”¨ softmax å‡½æ•°è·å¾—æ³¨æ„åŠ›æƒé‡
attention_weights = F.softmax(attention_scores, dim=-1)

# è®¡ç®— Value å‘é‡çš„åŠ æƒå’Œ
output = torch.matmul(attention_weights, value)

print("è‡ªæ³¨æ„åŠ›æœºåˆ¶åçš„è¾“å‡º:")
print(output)
```

âœ… éå¸¸æ˜“äºç†è§£å’Œæ•™å­¦æ¼”ç¤ºã€‚

âŒ æ²¡æœ‰æ”¯æŒ `mask`ï¼Œæ— æ³•ç”¨äº NLP ä¸­å¸¸è§çš„ padding æˆ– causalã€‚

âŒ æ²¡æœ‰ dropoutã€‚

âŒ æ— æ³•å¤„ç† batched è¾“å…¥ã€‚

âŒ æ²¡æœ‰ GQAã€å¤šå¤´æ³¨æ„åŠ›ã€åˆ†å—è®¡ç®—ç­‰é«˜çº§åŠŸèƒ½ã€‚

âŒ æ²¡æœ‰åŒºåˆ†è®­ç»ƒ/æ¨ç†æ¨¡å¼ï¼ˆæ¯”å¦‚ dropoutï¼‰ã€‚

#### 1.5.3 å·¥ç¨‹çº§åˆ«å®ç°

å®ç°ä¸€ï¼š[adore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Modules.py#L7)

```python
class ScaledDotProductAttention(nn.Module):
    ''' Scaled Dot-Product Attention '''

    def __init__(self, temperature, attn_dropout=0.1):
        super().__init__()
        # ç¼©æ”¾å› å­
        self.temperature = temperature
        # åŠ å…¥äº†æ­£åˆ™åŒ–
        self.dropout = nn.Dropout(attn_dropout)

    def forward(self, q, k, v, mask=None):

        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))
	    # å°†maskä¸­ä¸º0çš„åœ°æ–¹çš„å€¼æ›¿æ¢ä¸º-1e9ï¼Œè¿™æ ·åœ¨softmaxçš„æ—¶å€™-1e9ä¼šå˜ä¸º0
        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e9)
        # å¯¹æ³¨æ„åŠ›æƒé‡åº”ç”¨dropout
        attn = self.dropout(F.softmax(attn, dim=-1))
        output = torch.matmul(attn, v)

        return output, attn
```

| ä¼˜åŒ–ç‚¹      | è¯´æ˜                                                         |
| ----------- | ------------------------------------------------------------ |
| ğŸ¯ æ¨¡å—åŒ–    | å°è£…æˆ PyTorch çš„ `nn.Module`ï¼Œä¾¿äºå¤ç”¨å’Œç»„åˆåˆ° Transformer æ¨¡å—ä¸­ |
| ğŸ¯ æ”¯æŒ mask | åŠ å…¥ `masked_fill(mask == 0, -1e9)`ï¼Œå¯ç”¨äº padding å’Œ causal mask |
| ğŸ¯ Dropout   | å¢åŠ éšæœºæ€§ï¼Œé¿å…è¿‡æ‹Ÿåˆ                                       |

â¡ï¸ é€‚åˆæ„å»ºè‡ªå®šä¹‰ Transformer æ¨¡å‹ï¼Œç”¨äºä¸­ç­‰è§„æ¨¡ä»»åŠ¡ã€‚

å®ç°äºŒï¼š[torch.nn.functional.scaled_dot_product_attention â€” PyTorch 2.7 documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention)

```python
# Efficient implementation equivalent to the following:
def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,
        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:
    # æå– Query å’Œ Key å¼ é‡çš„é•¿åº¦ç»´åº¦ï¼Œç”¨äºåç»­æ„å»ºæ³¨æ„åŠ›çŸ©é˜µå’Œ Mask
    L, S = query.size(-2), key.size(-2)
    # æ”¯æŒé»˜è®¤ï¼Œä¹Ÿæ”¯æŒè‡ªå®šç¼©æ”¾å› å­
    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale
    # æ”¯æŒbias
    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)
    # å› æœé®æŒ¡ã€‚
    # åœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶ï¼Œæ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°è‡ªå·±ä»¥åŠä¹‹å‰çš„ tokenï¼Œä¸èƒ½â€œå·çœ‹æœªæ¥â€ã€‚æ‰€ä»¥è¦æ„é€ ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼ˆtril()ï¼‰ï¼ŒæŠŠä¸Šä¸‰è§’éƒ¨åˆ†å¡«æˆ -infï¼ˆç»è¿‡ softmax åæ¥è¿‘ 0ï¼‰ï¼Œæ³¨æ„åŠ›ç½®ä¸º0ã€‚
    if is_causal:
        assert attn_mask is None
        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)
        attn_bias.masked_fill_(temp_mask.logical_not(), float("-inf"))
        attn_bias.to(query.dtype)
	# å…è®¸è‡ªå®šä¹‰mask
    if attn_mask is not None:
        if attn_mask.dtype == torch.bool:
            attn_bias.masked_fill_(attn_mask.logical_not(), float("-inf"))
        else:
            attn_bias = attn_mask + attn_bias
	# æ”¯æŒåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›Grouped Query Attentionï¼ˆGQAï¼‰
    # GQA è®©ä½ ç”¨æ›´å°‘çš„ Key/Value ç»„å»æœåŠ¡æ›´å¤šçš„ Queryï¼Œæ¯”å¦‚ï¼šQuery æ˜¯ 16 ä¸ªå¤´ï¼ŒKey/Value åªæœ‰ 4 ä¸ªå¤´ï¼Œå°±é€šè¿‡ repeat_interleave å¤åˆ¶ Key/Value æ¥é€‚é… Queryï¼Œè¿™æ ·å¯ä»¥ å‡å°‘è®¡ç®—é‡ä¸å†…å­˜ï¼Œä½†ä¿ç•™å¤§å®¹é‡ Query çš„çµæ´»æ€§ã€‚
    if enable_gqa:
        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)
        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)

    attn_weight = query @ key.transpose(-2, -1) * scale_factor
    attn_weight += attn_bias
    attn_weight = torch.softmax(attn_weight, dim=-1)
    # åªæœ‰trainingçš„æ—¶å€™æ‰dropout
    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)
    return attn_weight @ value
```



## 2. å¤šå¤´æ³¨æ„åŠ›

â€œå¤šå¤´æ³¨æ„åŠ›â€è¿™ä¸ªæ¦‚å¿µæ˜¯**ç»“æ„å½¢å¼**ï¼Œå®ƒæè¿°çš„æ˜¯ï¼š

> æŠŠæ³¨æ„åŠ›æœºåˆ¶å¹¶è¡Œåˆ†æˆå¤šä¸ªâ€œå¤´â€ï¼Œæ¯ä¸ªå¤´ä½¿ç”¨ä¸åŒçš„å‚æ•°æ¥æå–ä¸åŒçš„ä¿¡æ¯ï¼Œç„¶åå†æŠŠå¤šä¸ªå¤´çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥ã€‚

> æ‰€ä»¥ï¼š**â€œå¤šå¤´â€æ˜¯æ–¹å¼ï¼Œâ€œæ³¨æ„åŠ›ç±»å‹â€æ˜¯å†…å®¹ã€‚**

æ‰€ä»¥å¯ä»¥æœ‰ï¼š

- å¤šå¤´çš„**è‡ªæ³¨æ„åŠ›**
- å¤šå¤´çš„**äº¤å‰æ³¨æ„åŠ›**
- å¤šå¤´çš„**å±€éƒ¨æ³¨æ„åŠ›**
- å¤šå¤´çš„**å› æœæ³¨æ„åŠ›**

æ¯ä¸ªå¤´éƒ½å¯ä»¥ç‹¬ç«‹å¤„ç†ä¸åŒä¿¡æ¯ï¼Œä½†å®ƒä»¬éµå¾ªåŒä¸€ä¸ªæ³¨æ„åŠ›è§„åˆ™ï¼ˆæ¯”å¦‚éƒ½ç”¨è‡ªæ³¨æ„åŠ›ï¼‰

> æ³¨æ„åŠ›ä¹‹äºå¤šå¤´æ³¨æ„åŠ›ï¼Œå¥½ä¼¼å•å·ç§¯æ ¸ä¹‹äºå¤šå·ç§¯æ ¸ã€‚

### 2.1 å¤šå¤´æ³¨æ„åŠ›çš„æ ¸å¿ƒæ€æƒ³

- Q/K/V è¢«æ‹†æˆ n å—ï¼Œnä¸ªå¤´**åœ¨ä¸åŒçš„å­ç©ºé—´ä¸­å»ºæ¨¡æ³¨æ„åŠ›**ï¼Œè¿™æ ·å¤šä¸ªå¤´å°±èƒ½ä»å¤šä¸ªæ–¹å‘ç†è§£ä¿¡æ¯ã€‚
- å†æŠŠå¤šä¸ªå¤´å¾—åˆ°çš„æ³¨æ„åŠ›æ‹¼æ¥å›å»ã€‚
- æœ€åé€šè¿‡ä¸€ä¸ªçº¿æ€§æŠ•å½±å±‚èåˆå’Œè¿˜åŸç»´åº¦ã€‚

### 2.2 å®ç°

#### 2.2.1 ä¼ªä»£ç 

```python
# è¾“å…¥:
# X: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, embed_dim)
# n_heads: æ³¨æ„åŠ›å¤´æ•°
# d_k: æ¯ä¸ªå¤´çš„ Query / Key çš„ç»´åº¦
# d_v: æ¯ä¸ªå¤´çš„ Value çš„ç»´åº¦
# W_q, W_k, W_v: æŠ•å½±çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (embed_dim, n_heads * d_k) / (embed_dim, n_heads * d_v)
# W_o: è¾“å‡ºæŠ•å½±çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (n_heads * d_v, embed_dim)

# Step 1: çº¿æ€§æ˜ å°„ï¼Œç”Ÿæˆ Q, K, V
Q = X @ W_q  # shape: (B, L, n_heads * d_k)
K = X @ W_k
V = X @ W_v

# Step 2: æ‹†åˆ†å¤šå¤´
Q = reshape(Q, shape=(B, L, n_heads, d_k)).transpose(1, 2)  # (B, n_heads, L, d_k)
K = reshape(K, shape=(B, L, n_heads, d_k)).transpose(1, 2)
V = reshape(V, shape=(B, L, n_heads, d_v)).transpose(1, 2)

# Step 3: å¯¹æ¯ä¸ªå¤´è®¡ç®—æ³¨æ„åŠ›
scores = (Q @ K.transpose(-2, -1)) / sqrt(d_k)  # (B, n_heads, L, L)
attn_weights = softmax(scores, dim=-1)
attn_output = attn_weights @ V  # (B, n_heads, L, d_v)

# Step 4: åˆå¹¶æ‰€æœ‰å¤´
attn_output = attn_output.transpose(1, 2).reshape(B, L, n_heads * d_v)

# Step 5: è¾“å‡ºçº¿æ€§æ˜ å°„
output = attn_output @ W_o  # (B, L, embed_dim)

```

#### 2.2.2 åŸç”Ÿå®ç°

```python
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

# å®šä¹‰ä¸€ä¸ª MultiHeadAttention ç±»ï¼Œå®ƒç»§æ‰¿è‡ª nn.Module
class MultiHeadAttention(nn.Module):
    def __init__(self, heads, d_model, dropout=0.1):
        # è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°
        super().__init__()
        # ä¿å­˜æ¨¡å‹ç»´åº¦å’Œå¤´æ•°
        self.d_model = d_model
        self.d_k = d_model // heads  # æ¯ä¸ªå¤´å¯¹åº”çš„ç»´åº¦
        self.h = heads  # å¤´çš„æ•°é‡

        # åˆå§‹åŒ–çº¿æ€§å±‚ï¼Œç”¨äºå°†è¾“å…¥è½¬æ¢ä¸ºæŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰å’Œå€¼ï¼ˆVï¼‰
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        # åˆå§‹åŒ–Dropoutå±‚ï¼Œç”¨äºæ­£åˆ™åŒ–
        self.dropout = nn.Dropout(dropout)
        # åˆå§‹åŒ–è¾“å‡ºçº¿æ€§å±‚ï¼Œç”¨äºå°†å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºè½¬æ¢ä¸ºæ¨¡å‹ç»´åº¦
        self.out = nn.Linear(d_model, d_model)

    # å®šä¹‰æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹
    def attention(self, q, k, v, mask=None):
        # è®¡ç®—Qå’ŒKçš„çŸ©é˜µä¹˜ç§¯ï¼Œç„¶åé™¤ä»¥æ ¹å·ä¸‹d_k
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        # å¦‚æœæä¾›äº†æ©ç ï¼Œåˆ™å°†æ©ç å¯¹åº”çš„ä½ç½®è®¾ç½®ä¸ºè´Ÿæ— ç©·ï¼Œè¿™æ ·åœ¨softmaxåè¿™äº›ä½ç½®çš„å€¼ä¸º0
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        # åº”ç”¨softmaxå‡½æ•°è·å¾—æ³¨æ„åŠ›æƒé‡
        scores = F.softmax(scores, dim=-1)
        # åº”ç”¨dropout
        scores = self.dropout(scores)
        # å°†æ³¨æ„åŠ›æƒé‡å’ŒVç›¸ä¹˜å¾—åˆ°è¾“å‡º
        output = torch.matmul(scores, v)
        return output

    # å®šä¹‰å‰å‘ä¼ æ’­è¿‡ç¨‹
    def forward(self, q, k, v, mask=None):
        batch_size = q.size(0)
        # å°†è¾“å…¥Qã€Kã€Vé€šè¿‡çº¿æ€§å±‚ï¼Œå¹¶è°ƒæ•´å½¢çŠ¶ä»¥è¿›è¡Œå¤šå¤´æ³¨æ„åŠ›è®¡ç®—
        q = self.q_linear(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        k = self.k_linear(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        v = self.v_linear(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        # è®¡ç®—æ³¨æ„åŠ›è¾“å‡º
        scores = self.attention(q, k, v, mask)
        # å°†å¤šå¤´è¾“å‡ºåˆå¹¶ï¼Œå¹¶è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…æ¨¡å‹ç»´åº¦
        concat = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        # é€šè¿‡è¾“å‡ºçº¿æ€§å±‚
        output = self.out(concat)
        return output
```

#### 2.2.3 å·¥ç¨‹çº§åˆ«å®ç°

å®ç°ä¸€ï¼š[jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py#L9)

```python
class MultiHeadAttention(nn.Module):
    ''' Multi-Head Attention module '''

    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):
        super().__init__()
        # d_model å°±æ˜¯åµŒå…¥ç»´åº¦ï¼ˆembedding dimensionï¼‰ï¼Œä¹Ÿå«åšæ¨¡å‹çš„éšçŠ¶æ€ç»´åº¦ï¼ˆhidden sizeï¼‰ï¼Œæ˜¯ Transformer ä¸­è´¯ç©¿å§‹ç»ˆçš„æ ¸å¿ƒç»´åº¦ã€‚

        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v

        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False) # Qè¦å’ŒKç‚¹ç§¯ï¼Œæ‰€ä»¥å…¶å®å¿…é¡»d_q = d_k
        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)
        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)
        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)

        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)

        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)


    def forward(self, q, k, v, mask=None):

        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head
        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)

        residual = q

        # Pass through the pre-attention projection: b x lq x (n*dv)
        # Separate different heads: b x lq x n x dv
        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)
        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)
        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)

        # Transpose for attention dot product: b x n x lq x dv
        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)

        if mask is not None:
            mask = mask.unsqueeze(1)   # For head axis broadcasting.

        q, attn = self.attention(q, k, v, mask=mask)

        # Transpose to move the head dimension back: b x lq x n x dv
        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)
        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)
        q = self.dropout(self.fc(q))
        q += residual

        q = self.layer_norm(q)

        return q, attn
```

å®ç°äºŒï¼š[PyTorch Tutorials 2.7.0+cu126 documentation](https://docs.pytorch.org/tutorials/intermediate/transformer_building_blocks.html#)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class MultiHeadAttention(nn.Module):
    """
    Computes multi-head attention. Supports nested or padded tensors.

    Args:
        E_q (int): Size of embedding dim for query
        E_k (int): Size of embedding dim for key
        E_v (int): Size of embedding dim for value
        E_total (int): Total embedding dim of combined heads post input projection. Each head
            has dim E_total // nheads
        nheads (int): Number of heads
        dropout (float, optional): Dropout probability. Default: 0.0
        bias (bool, optional): Whether to add bias to input projection. Default: True
    """

    def __init__(
        self,
        E_q: int,
        E_k: int,
        E_v: int,
        E_total: int,
        nheads: int,
        dropout: float = 0.0,
        bias=True,
        device=None,
        dtype=None,
    ):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.nheads = nheads
        self.dropout = dropout
        self._qkv_same_embed_dim = E_q == E_k and E_q == E_v
        if self._qkv_same_embed_dim:
            self.packed_proj = nn.Linear(E_q, E_total * 3, bias=bias, **factory_kwargs)
        else:
            self.q_proj = nn.Linear(E_q, E_total, bias=bias, **factory_kwargs)
            self.k_proj = nn.Linear(E_k, E_total, bias=bias, **factory_kwargs)
            self.v_proj = nn.Linear(E_v, E_total, bias=bias, **factory_kwargs)
        E_out = E_q
        self.out_proj = nn.Linear(E_total, E_out, bias=bias, **factory_kwargs)
        assert E_total % nheads == 0, "Embedding dim is not divisible by nheads"
        self.E_head = E_total // nheads
        self.bias = bias

    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attn_mask=None,
        is_causal=False,
    ) -> torch.Tensor:
        """
        Forward pass; runs the following process:
            1. Apply input projection
            2. Split heads and prepare for SDPA
            3. Run SDPA
            4. Apply output projection

        Args:
            query (torch.Tensor): query of shape (``N``, ``L_q``, ``E_qk``)
            key (torch.Tensor): key of shape (``N``, ``L_kv``, ``E_qk``)
            value (torch.Tensor): value of shape (``N``, ``L_kv``, ``E_v``)
            attn_mask (torch.Tensor, optional): attention mask of shape (``N``, ``L_q``, ``L_kv``) to pass to SDPA. Default: None
            is_causal (bool, optional): Whether to apply causal mask. Default: False

        Returns:
            attn_output (torch.Tensor): output of shape (N, L_t, E_q)
        """
        # Step 1. Apply input projection
        if self._qkv_same_embed_dim:
            if query is key and key is value:
                result = self.packed_proj(query)
                query, key, value = torch.chunk(result, 3, dim=-1)
            else:
                q_weight, k_weight, v_weight = torch.chunk(
                    self.packed_proj.weight, 3, dim=0
                )
                if self.bias:
                    q_bias, k_bias, v_bias = torch.chunk(
                        self.packed_proj.bias, 3, dim=0
                    )
                else:
                    q_bias, k_bias, v_bias = None, None, None
                query, key, value = (
                    F.linear(query, q_weight, q_bias),
                    F.linear(key, k_weight, k_bias),
                    F.linear(value, v_weight, v_bias),
                )

        else:
            query = self.q_proj(query)
            key = self.k_proj(key)
            value = self.v_proj(value)

        # Step 2. Split heads and prepare for SDPA
        # reshape query, key, value to separate by head
        # (N, L_t, E_total) -> (N, L_t, nheads, E_head) -> (N, nheads, L_t, E_head)
        query = query.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)
        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)
        key = key.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)
        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)
        value = value.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)

        # Step 3. Run SDPA
        # (N, nheads, L_t, E_head)
        attn_output = F.scaled_dot_product_attention(
            query, key, value, dropout_p=self.dropout, is_causal=is_causal
        )
        # (N, nheads, L_t, E_head) -> (N, L_t, nheads, E_head) -> (N, L_t, E_total)
        attn_output = attn_output.transpose(1, 2).flatten(-2)

        # Step 4. Apply output projection
        # (N, L_t, E_total) -> (N, L_t, E_out)
        attn_output = self.out_proj(attn_output)

        return attn_output
```



[åˆ†ætransformeræ¨¡å‹çš„å‚æ•°é‡ã€è®¡ç®—é‡ã€ä¸­é—´æ¿€æ´»ã€KV cache - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/624740065)